{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aad6e64-df24-4690-bc11-d6c5eb5a514e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Part A of the script\n",
    "\n",
    "#Load the dataset.\n",
    "#Load and prepare the model and tokenizer.\n",
    "#Save the dataset, model, and tokenizer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bbc191-21be-43fe-a118-cf3d1136cef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m167.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m330.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m323.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m323.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m315.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m319.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m281.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (16.1.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m262.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow-hotfix, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "Successfully installed datasets-2.20.0 huggingface-hub-0.23.4 multiprocess-0.70.16 pyarrow-hotfix-0.6 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.4 xxhash-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df9a152-799f-4294-98f3-c233ea8d5fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: Tesla T4\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c79bf8-ecee-445b-b3ad-f253829109d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853d436f-27cd-4b1c-ad21-91877c8ea7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the HumanEval-X datasets...\n",
      "Combining dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load the HumanEval-X dataset for all languages\n",
    "print(\"Loading the HumanEval-X datasets...\")\n",
    "humaneval_x_dataset_py = load_dataset(\"THUDM/humaneval-x\", \"python\")\n",
    "humaneval_x_dataset_cpp = load_dataset(\"THUDM/humaneval-x\", \"cpp\")\n",
    "humaneval_x_dataset_go = load_dataset(\"THUDM/humaneval-x\", \"go\")\n",
    "humaneval_x_dataset_java = load_dataset(\"THUDM/humaneval-x\", \"java\")\n",
    "humaneval_x_dataset_js = load_dataset(\"THUDM/humaneval-x\", \"js\")\n",
    "# humaneval_x_dataset_rust = load_dataset(\"THUDM/humaneval-x\", \"rust\")\n",
    "\n",
    "# Combine the datasets\n",
    "print(\"Combining dataset...\")\n",
    "combined_dataset = concatenate_datasets([  humaneval_x_dataset_py[\"test\"], humaneval_x_dataset_cpp[\"test\"],\n",
    "                                         humaneval_x_dataset_go[\"test\"],humaneval_x_dataset_java[\"test\"] ,\n",
    "                                         humaneval_x_dataset_js[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62082c5c-fccf-49fa-a65f-c56612fda941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the HumanEval-XL datasets...\n",
      "Dataset({\n",
      "    features: ['task_id', 'language', 'prompt', 'description', 'test', 'entry_point', 'canonical_solution', 'natural_language'],\n",
      "    num_rows: 22080\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the HumanEval-XL dataset for all languages\n",
    "print(\"Loading the HumanEval-XL datasets...\")\n",
    "\n",
    "programming_languages = [\n",
    "    \"python\", \"go\", \"java\", \"javascript\", \"scala\", \"csharp\",\n",
    "     \"kotlin\", \"php\", \"perl\", \"ruby\", \"swift\", \"typescript\"\n",
    "]\n",
    "# 23 natural languages each having 80 rows of data makes up 1840 rows of data per programming language\n",
    "\n",
    "# List to collect datasets\n",
    "datasets = []\n",
    "\n",
    "for programming_language in programming_languages:\n",
    "    programming_lang_dataset = load_dataset(\"FloatAI/HumanEval-XL\", programming_language)\n",
    "    \n",
    "    concatenated_dataset = concatenate_datasets( [part for part in programming_lang_dataset.values()])\n",
    "    datasets.append(concatenated_dataset)\n",
    "\n",
    "#print(datasets)\n",
    "# Optionally concatenate all datasets into one\n",
    "combined_dataset = concatenate_datasets(datasets)\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1f6e15-4cf8-4ba5-8cc9-134652bfbc72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined dataset...\n"
     ]
    }
   ],
   "source": [
    "# Save combined dataset\n",
    "print(\"Saving combined dataset...\")\n",
    "torch.save(combined_dataset, \"combined_dataset.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4d39cc9-461e-4da3-906b-9e78234ebbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94e22ac1-be97-400b-bdbb-2415027e580b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "print(\"Saving model and tokenizer...\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83496c3a-ac43-494a-9a59-d8415b52904e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_code(prompt, model, tokenizer, device, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process to keep only the main function and ensure it ends properly\n",
    "    if \"def \" in generated_code:\n",
    "        # Extract the main function including everything after it\n",
    "        main_function = generated_code.split(\"def \", 1)[1]\n",
    "        main_function = \"def \" + main_function  # Re-add the def keyword\n",
    "        \n",
    "        # Detect end of function by finding the next function or end of indentation\n",
    "        lines = main_function.split('\\n')\n",
    "        function_lines = []\n",
    "        inside_function = False\n",
    "        indentation_level = None\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line.startswith(\"def \") and inside_function:\n",
    "                break  # New function starts, stop collecting lines\n",
    "            if not inside_function:\n",
    "                if stripped_line.startswith(\"def \"):\n",
    "                    inside_function = True\n",
    "                    indentation_level = len(line) - len(line.lstrip())\n",
    "            if inside_function:\n",
    "                if stripped_line == \"\" or line.startswith(\" \" * indentation_level):\n",
    "                    function_lines.append(line)\n",
    "                else:\n",
    "                    break  # End of the function when the indentation changes\n",
    "\n",
    "        complete_function = \"\\n\".join(function_lines)\n",
    "        return complete_function\n",
    "    return generated_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f9436e8-07b0-42d6-8960-2f4be3b278c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the generate_code function\n",
    "import dill\n",
    "with open(\"generate_code.pkl\", \"wb\") as f:\n",
    "    dill.dump(generate_code, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3556b-c73f-4a14-91fd-0e090efc4475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
